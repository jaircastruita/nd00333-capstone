{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using HyperDrive\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Environment\n",
        "from azureml.core import Experiment\n",
        "from azureml.core.dataset import Dataset\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "from azureml.core import ScriptRunConfig\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "from azureml.train.hyperdrive import BayesianParameterSampling, HyperDriveConfig, PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive import normal, uniform, choice\n",
        "\n",
        "import shutil\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613612854998
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"https://raw.githubusercontent.com/jaircastruita/nd00333-capstone/master/starter_file/data/WA_Fn-UseC_-HR-Employee-Attrition.csv\"\n",
        "\n",
        "ibm_ds = Dataset.Tabular.from_delimited_files(path=data_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613612870436
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "experiment_name = 'dnn-experiment'\n",
        "\n",
        "experiment=Experiment(ws, experiment_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613612874154
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Target"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_name = \"hd-dnn-cluster\"\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print(\"Reusing already created compute target\")\n",
        "\n",
        "except ComputeTargetException:\n",
        "    print(\"Creating compute target...\")\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\", max_nodes=6)\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "compute_target.wait_for_completion(show_output=True)\n",
        "print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613612904178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_folder = \"./dnn-attrition\"\n",
        "os.makedirs(project_folder, exist_ok=True)\n",
        "shutil.copy(\"dnn-train.py\", project_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613612909798
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperdrive Configuration\n",
        "\n",
        "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598531923519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\r\n",
        "\r\n",
        "channels:\r\n",
        "- conda-forge\r\n",
        "dependencies:\r\n",
        "- python=3.6.2\r\n",
        "- pip:\r\n",
        "  - azureml-defaults\r\n",
        "  - torch==1.6.0\r\n",
        "  - scikit-learn"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create an early termination policy. This is not required if you are using Bayesian sampling.\n",
        "# early_termination_policy = <your policy here>\n",
        "\n",
        "param_sampling = BayesianParameterSampling({\n",
        "    \"learning_rate\": uniform(0.001, 0.1),\n",
        "    \"num_epochs\": choice(range(100, 5000)),\n",
        "    \"layer1\": choice(range(10, 100)),\n",
        "    \"layer2\": choice(range(10, 100)),\n",
        "})\n",
        "\n",
        "conda_env = Environment.from_conda_specification(name=\"PyTorch-1.6-GPU-scikit-learn\", file_path=\"conda_dependencies.yml\")\n",
        "# conda_env = Environment.get(ws, \"AzureML-PyTorch-1.6-GPU\").clone(\"PyTorch-1.6-GPU-scikit-learn\")\n",
        "\n",
        "# for conda_package in [\"scikit-learn\"]:\n",
        "#     conda_env.python.conda_dependencies.add_conda_package(conda_package)\n",
        "\n",
        "# Specify a GPU base image\n",
        "conda_env.docker.enabled = True\n",
        "conda_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
        "\n",
        "src = ScriptRunConfig(source_directory=project_folder,\n",
        "                      script=\"dnn-train.py\",\n",
        "                      arguments=[\"--num_epochs\", 100, \"--layer1\", 30, \"--layer2\", 15, \"--learning_rate\", 0.001,\"--output_dir\", \"./outputs\"],\n",
        "                      compute_target=compute_target,\n",
        "                      environment=conda_env)\n",
        "\n",
        "hyperdrive_run_config = HyperDriveConfig(run_config=src, \n",
        "                                         hyperparameter_sampling=param_sampling, \n",
        "                                         primary_metric_name='best_test_acc', \n",
        "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                         max_total_runs=8,\n",
        "                                         max_concurrent_runs=4)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613613299380
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run = experiment.submit(src)\n",
        "# print(run)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613508900584
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RunDetails(run).show()\n",
        "# run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613509332425
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Details\n",
        "\n",
        "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
        "\n",
        "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598544898497
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperdrive_run = experiment.submit(hyperdrive_run_config)\n",
        "RunDetails(hyperdrive_run).show()\n",
        "hyperdrive_run.wait_for_completion(show_output=True)\n",
        "# assert(hyperdrive_run.get_status == \"Completed\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613615240161
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert(hyperdrive_run.get_status() == 'Completed')"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613615852707
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Model\n",
        "\n",
        "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "best_run_metrics = best_run.get_metrics()\n",
        "print(best_run)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run(Experiment: dnn-experiment,\n",
            "Id: HD_49531712-912e-4551-8c7d-99d490039108_6,\n",
            "Type: azureml.scriptrun,\n",
            "Status: Completed)\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1613616516493
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Best Run is:\\n  Validation accuracy: {} \\n  Learning rate: {} \\n Number epochs: {} \\n Layer 1: {} \\n layer 2: {}'.format(\n",
        "        best_run_metrics['best_test_acc'][-1],\n",
        "        best_run_metrics['learning_rate'],\n",
        "        best_run_metrics['num_epochs'],\n",
        "        best_run_metrics['num_layer1'],\n",
        "        best_run_metrics['num_layer2'])\n",
        "     )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Run is:\n",
            "  Validation accuracy: 0.7102678955982428 \n",
            "  Learning rate: 0.06610978015992655 \n",
            " Number epochs: 4298 \n",
            " Layer 1: 56 \n",
            " layer 2: 64\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1613616573891
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = best_run.register_model(model_name='pytorch-attrition', \n",
        "                                model_path='outputs/model.pt')\n",
        "print(model.name, model.id, model.version, sep='\\t')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch-attrition\tpytorch-attrition:1\t1\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1613616606047
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment\n",
        "\n",
        "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
        "\n",
        "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from azureml.core.environment import Environment\r\n",
        "# from azureml.core.model import InferenceConfig\r\n",
        "\r\n",
        "\r\n",
        "# env = Environment.get(workspace, \"AzureML-Minimal\").clone(env_name)\r\n",
        "\r\n",
        "# for pip_package in [\"scikit-learn\"]:\r\n",
        "#     env.python.conda_dependencies.add_pip_package(pip_package)\r\n",
        "\r\n",
        "# inference_config = InferenceConfig(entry_script='path-to-score.py',\r\n",
        "#                                     environment=env)\r\n",
        "\r\n",
        "# # Set deployment configuration\r\n",
        "# deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, \r\n",
        "#                                                        memory_gb=1,\r\n",
        "#                                                        tags={'area': \"AttritionData\", 'type': \"hd_dnn_classification\"},\r\n",
        "#                                                        description='sample service for HyperDrive Classification')\r\n",
        "\r\n",
        "# # Define the model, inference, & deployment configuration and web service name and location to deploy\r\n",
        "# service = Model.deploy(\r\n",
        "#     workspace=ws,\r\n",
        "#     name=\"automl-web-service\",\r\n",
        "#     models=[model],\r\n",
        "#     inference_config=inference_config,\r\n",
        "#     deployment_config=deployment_config)\r\n",
        "\r\n",
        "# service.wait_for_deployment(show_output=True)\r\n",
        "# print(service.state)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: In the cell below, send a request to the web service you deployed to test it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: In the cell below, print the logs of the web service and delete the service"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}